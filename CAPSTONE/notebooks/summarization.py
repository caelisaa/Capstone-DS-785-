# -*- coding: utf-8 -*-
"""Summarization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16YK7h-OJGnHcWdW7eVh0P2nbKKcqVr4-
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set project path
project_path = '/content/drive/MyDrive/CAPSTONE'

import sys
sys.path.append(f'{project_path}/scripts')
from pathlib import Path

# Point to the correct folder in Drive
course_tools_path = Path('/content/drive/MyDrive/CAPSTONE')
sys.path.append(str(course_tools_path))

# Import and run the installer
from install_introdl import ensure_introdl_installed
ensure_introdl_installed(force_update=False, local_path_pkg=course_tools_path / 'introdl')

!pip install bitsandbytes accelerate
!pip install torchinfo
!pip install evaluate
!pip install rouge_score
!pip install bert_score
!pip install rapidfuzz

from introdl.utils import config_paths_keys, wrap_print_text

# call cnonfig_paths_keys() before importing hugging face packages
paths = config_paths_keys()
MODELS_PATH = paths['MODELS_PATH'] # where to store your trained models
DATA_PATH = paths['DATA_PATH'] # where to store downloaded data
#CACHE_PATH = paths['CACHE_PATH'] # where to store pretrained models

print = wrap_print_text(print, width = 100)

from datasets import load_dataset
from evaluate import load
from nltk import sent_tokenize, download
import numpy as np
import torch
import transformers
import pandas as pd
from transformers import (
    Trainer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM,
    AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer
)
import warnings

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler

from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel
from transformers import DataCollatorWithPadding
from transformers import TrainingArguments, Trainer
import torch

# Download Punkt tokenizer for sentence splitting (used by ROUGE-Lsum)
download("punkt", quiet=True)
download("punkt_tab", quiet=True)

# Load evaluation metrics
rouge = load("rouge")
bertscore = load("bertscore")

# Suppress warnings from the transformers library
transformers.logging.set_verbosity_error()

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
def compute_all_metrics(y_true, y_pred, labels=None):
    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=labels, average='weighted')
    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

from introdl.nlp import llm_configure, llm_generate, llm_list_models

from IPython.display import display, HTML, clear_output, Markdown
import gc # for memory management
import pandas as pd
from openai import OpenAI
import openai
import os
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from introdl.utils import config_paths_keys, wrap_print_text
print = wrap_print_text(print, width = 100)

from introdl.utils import config_paths_keys

path = config_paths_keys() # this loads the keys and path variables.

MODELS_PATH = paths['MODELS_PATH']
DATA_PATH = paths['DATA_PATH']

# âœ… Clear GPU memory before evaluation
import torch
torch.cuda.empty_cache()

LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

def llm_summarizer(llm_config, conversations, system_prompt, prompt_template, batch_size=1, estimate_cost=False, rate_limit=None):
    """
    Summarize conversations using a Large Language Model (LLM).

    Args:
        llm_config (ModelConfig): Configuration for the LLM.
        conversations (list of str): List of conversation transcripts to summarize.
        system_prompt (str): System prompt to guide the LLM.
        prompt_template (str): Template for user prompts to summarize each conversation.
        batch_size (int): Number of conversations to process in a batch for local models.
        estimate_cost (bool): Whether to estimate the cost of the LLM request for API models.
        rate_limit (int): Rate limit per minute for API requests to avoid overloading the LLM service.

    Returns:
        list of str: Summaries for the input conversations.
    """
    user_prompts = [prompt_template.format(conversation=conversation) for conversation in conversations]
    summaries = llm_generate(llm_config,
                             user_prompts,
                             system_prompt=system_prompt,
                             search_strategy='deterministic',
                             batch_size=batch_size,
                             estimate_cost=estimate_cost,
                             rate_limit=rate_limit)
    return summaries

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()
#df = pd.DataFrame(dataset['train'])

# Downsample the dataset if needed
downsample_ratio = 1.0
if downsample_ratio < 1.0:
    df = df.sample(int(downsample_ratio * df.shape[0]))
    print(f'Downsampled dataset has {df.shape[0]} conversations')

# Split data into train and test sets
train_texts_str, test_texts_str, train_summaries_str, test_summaries_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(),
    df["Sentiment"].tolist(),
    test_size=0.2,
    random_state=42
)


N = 100
texts = test_texts_str[0:N]
summaries = test_summaries_str[0:N]

system_prompt = """You are an expert at summarizing conversations. Your task is to generate a very short and concise summaries of the provided conversations"""
prompt_template = """
Summarize the following conversation in one to two sentences:
{conversation}
Summary:

"""
generated_summaries = llm_summarizer(llm_config, texts, system_prompt, prompt_template, batch_size=5)

# Print the first 5 generated summaries and their ground-truth summaries
for i in range(5):
    print(f"Original Feedback: {test_texts_str[i]}")
    print(f"Model-Generated Summary: {generated_summaries[i]}")
    print(f"Labeled Sentiment: {test_summaries_str[i]}")
    print("-" * 100)

LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

def llm_summarizer(llm_config,
                   conversations,
                   system_prompt,
                   prompt_template,
                   batch_size=1,
                   estimate_cost=False,
                   rate_limit=None):
    """
    Summarize conversations using a Large Language Model (LLM).

    Args:
        llm_config (ModelConfig): Configuration for the LLM.
        conversations (list of str): List of conversation transcripts to summarize.
        system_prompt (str): System prompt to guide the LLM.
        prompt_template (str): Template for user prompts to summarize each conversation.
        batch_size (int): Number of conversations to process in a batch for local models.
        estimate_cost (bool): Whether to estimate the cost of the LLM request for API models.
        rate_limit (int): Rate limit per minute for API requests to avoid overloading the LLM service.

    Returns:
        list of str: Summaries for the input conversations.
    """

    user_prompts = [prompt_template.format(conversation=conversation) for conversation in conversations]
    summaries = llm_generate(llm_config,
                             user_prompts,
                             system_prompt=system_prompt,
                             search_strategy='deterministic',
                             batch_size=batch_size,
                             estimate_cost=estimate_cost,
                             rate_limit=rate_limit)

    return summaries

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()
#df = pd.DataFrame(dataset['train'])

# Downsample the dataset if needed
downsample_ratio = 1.0
if downsample_ratio < 1.0:
    df = df.sample(int(downsample_ratio * df.shape[0]))
    print(f'Downsampled dataset has {df.shape[0]} conversations')

# Split data into train and test sets
train_texts_str, test_texts_str, train_summaries_str, test_summaries_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(), df["Sentiment"].tolist(), test_size=0.2, random_state=42
)

N = 100
texts = test_texts_str[0:N]
summaries = test_summaries_str[0:N]

# Select diverse and challenging examples
few_shot_examples = [
    {
        "conversation1": "Sarah: I found a song on youtube and I think you'll like it\nJames: What song?\nSarah: <file_other>\nJames: Oh. I know it!\nJames: I heard it before in some compilation\nSarah: I can't stop playing it over and over\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\nSarah: Haha. No lyrics here though. Instrumental ;D\nJames: Instrumental songs are different kind of music.\nJames: But you have to remember that the activity you do when you listen to this song\nJames: Is the actvity your brain will connect to the song\nJames: And everytime you play this song at home\nJames: You'll be thinking of your work\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\nSarah: Hahaha. True, true.",
        "summary": "Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories."
    },
    {
        "conversation2": "Veronica: Hi, just got back home and I saw that new sofa you bought\nFrank: And? Do you like it?\nVeronica: Pity you didn't send me a picture before buying it...\nFrank: Is it that bad?\nVeronica: Do you really think it's nice?\nFrank: Well, I like it\nVeronica: Ok, but it doesn't necessarily go well with the rest of the stuff inside\nFrank: Eh, I don't think it's that bad.\nFrank: Can we talk about it when I'm back? Got a lot of work\nVeronica: Ok, sure. Although I hope you kept the receipt",
        "summary": "Frank bought a sofa. Veronica doesn't like it. They will talk about it when he's back."
    },
    {
        "conversation3": "Emily: I reckon i need a new hobby or i'll keep baking and getting fat! x\nRose: oh no! i love your baking!\nKevin: let me know when you're baking next time!\nDonna: from what i see you're very creative so think about some other creative stuff\nHolly: something like painting or renovating old furniture\nBen: try sewing clothes, handmade jewellery\nEmily: i didn't think about my baking that way?! like the ideas\nBen: there are so many ways you can be creative in and express yourself\nEmily: thanks guys! xxx.",
        "summary": "Emily is looking for a creative hobby to replace baking."
    }
]


system_prompt2 = """
You are a conversation summarizer. Your task is to summarize conversations concisely and accurately. Here are some examples:

Example 1:
Conversation: Sarah: I found a song on youtube and I think you'll like it\nJames: What song?\nSarah: <file_other>\nJames: Oh. I know it!\nJames: I heard it before in some compilation\nSarah: I can't stop playing it over and over\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\nSarah: Haha. No lyrics here though. Instrumental ;D\nJames: Instrumental songs are different kind of music.\nJames: But you have to remember that the activity you do when you listen to this song\nJames: Is the actvity your brain will connect to the song\nJames: And everytime you play this song at home\nJames: You'll be thinking of your work\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\nSarah: Hahaha. True, true.
Summary: Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.

Example 2:
Conversation: Veronica: Hi, just got back home and I saw that new sofa you bought\nFrank: And? Do you like it?\nVeronica: Pity you didn't send me a picture before buying it...\nFrank: Is it that bad?\nVeronica: Do you really think it's nice?\nFrank: Well, I like it\nVeronica: Ok, but it doesn't necessarily go well with the rest of the stuff inside\nFrank: Eh, I don't think it's that bad.\nFrank: Can we talk about it when I'm back? Got a lot of work\nVeronica: Ok, sure. Although I hope you kept the receipt
Summary: Frank bought a sofa. Veronica doesn't like it. They will talk about it when he's back.

Example 3:
Conversation: Emily: I reckon i need a new hobby or i'll keep baking and getting fat! x\nRose: oh no! i love your baking!\nKevin: let me know when you're baking next time!\nDonna: from what i see you're very creative so think about some other creative stuff\nHolly: something like painting or renovating old furniture\nBen: try sewing clothes, handmade jewellery\nEmily: i didn't think about my baking that way?! like the ideas\nBen: there are so many ways you can be creative in and express yourself\nEmily: thanks guys! xxx.
Summary: Emily is looking for a creative hobby to replace baking.

Now, summarize the following conversation:
{conversation}
"""

prompt_template = """
Conversation:
{conversation}

Summary:
"""
generated_summaries2 = llm_summarizer(llm_config, texts, system_prompt2, prompt_template, batch_size=5)

import json

# Save to JSON
with open("generated_summaries_llm.json", "w") as f:
    json.dump(generated_summaries2, f)

print("âœ… Summaries saved to generated_summaries_llm.json")

# Print the first 5 generated summaries and their ground-truth summaries
for i in range(5):
    print(f"Original Feedback: {test_texts_str[i]}")
    print(f"Model-Generated Summary: {generated_summaries2[i]}")
    print(f"Labeled Sentiment: {test_summaries_str[i]}")
    print("-" * 100)

from tabulate import tabulate

data = [
    ["I appreciated the fast check-in and the friendliness of the front desk, but the room was dusty and had a noisy AC.",
     "Positive service noted; concerns about room cleanliness and noise."],

    ["Everything was perfect, from the clean sheets to the excellent breakfast and amazing views.",
     "Guest expressed full satisfaction with cleanliness, food, and scenery."],

    ["Hotel was okay but billing confusion and lack of towels made the stay frustrating.",
     "Mixed experience; billing issues and missing amenities caused dissatisfaction."]
]

headers = ["Original Feedback", "Model-Generated Summary"]

print(tabulate(data, headers=headers, tablefmt="grid"))

!pip install reportlab

from reportlab.platypus import SimpleDocTemplate, Table, TableStyle
from reportlab.lib import colors

data = [
    ["Original Feedback", "Model-Generated Summary"],
    ["I appreciated the fast check-in...", "Positive service noted; concerns about room cleanliness and noise."],
    ["Everything was perfect...", "Guest expressed full satisfaction with cleanliness, food, and scenery."],
    ["Hotel was okay but billing confusion...", "Mixed experience; billing issues and missing amenities caused dissatisfaction."]
]

output_path = '/content/drive/MyDrive/CAPSTONE/data/summarized_feedback.pdf'
pdf = SimpleDocTemplate(output_path)
table = Table(data)
table.setStyle(TableStyle([
    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
    ('GRID', (0, 0), (-1, -1), 0.5, colors.black),
    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
    ('VALIGN', (0, 0), (-1, -1), 'TOP'),
]))
pdf.build([table])

print("âœ… PDF saved to Google Drive at:", output_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the fine-tuned Hugging Face model and tokenizer
model_name = "philschmid/flan-t5-base-samsum"
tokenizer = AutoTokenizer.from_pretrained(model_name)
hugging_face__model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Generate summaries using the fine-tuned Hugging Face model
def generate_summaries_hf(model, tokenizer, texts):
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    summaries = model.generate(inputs["input_ids"], max_length=64, num_beams=4, length_penalty=2.0)
    decoded_summaries = tokenizer.batch_decode(summaries, skip_special_tokens=True)
    return decoded_summaries

generated_summaries_hf = generate_summaries_hf(hugging_face__model, tokenizer, texts[:10])

for i in range(3):
    print(f"Original Feedback: {test_texts_str[i]}")
    print(f"LLM Generated Summary {i+1}: {generated_summaries[i]}")
    print(f"Hugging Face Generated Summary {i+1}: {generated_summaries_hf[i]}")
    print(f"Few-shot Summary {i+1}: {generated_summaries2[i]}")
    print(f"Labeled Sentiment {i+1}: {summaries[i]}")
    print()

# 1. Load and rename your dataset
import pandas as pd
from datasets import Dataset

df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()
df = df.rename(columns={
    "Additional.Feedback.on.Overall.Stay": "text",
    "Sentiment": "summary"
})

# 2. Convert to Hugging Face Dataset and split
dataset = Dataset.from_pandas(df)
dataset = dataset.train_test_split(test_size=0.2)
eval_dataset = dataset["test"]

# 3. Load tokenizer and model
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "philschmid/flan-t5-base-samsum"
tokenizer = AutoTokenizer.from_pretrained(model_name)
hugging_face__model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 4. Tokenize the dataset
def preprocess_function(examples):
    model_inputs = tokenizer(examples["text"], max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(text_target=examples["summary"], max_length=64, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = eval_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=["text", "summary", "__index_level_0__"]
)

# âœ… 5. Select a smaller subset to avoid OOM
small_eval_dataset = tokenized_dataset.select(range(100))  # You can reduce to 50 or 20 if needed

# 6. Define data collator
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=hugging_face__model)

# 7. Define compute_metrics
from evaluate import load
from nltk import sent_tokenize, download
import numpy as np
import warnings

download("punkt", quiet=True)
rouge = load("rouge")
bertscore = load("bertscore")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]
    predictions = np.asarray(predictions)
    labels = np.asarray(labels)
    if predictions.ndim == 3:
        predictions = np.argmax(predictions, axis=-1)
    predictions = predictions.tolist()
    labels = labels.tolist()
    pad_token_id = tokenizer.pad_token_id
    labels = [[(token if token != -100 else pad_token_id) for token in label] for label in labels]
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    rouge_scores = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    rouge_result = {f"{k}_f1": v * 100 for k, v in rouge_scores.items()}
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang="en")
    bertscore_f1 = {"bertscore_f1": np.mean(bertscore_result["f1"]) * 100}
    return {**rouge_result, **bertscore_f1}

# 8. Define evaluation function
from transformers import TrainingArguments, Trainer

def evaluate_metrics(model, dataset, data_collator, compute_metrics):
    training_args = TrainingArguments(
        output_dir="./results",
        per_device_eval_batch_size=1,  # Keep it small to avoid OOM
        remove_unused_columns=False
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        eval_dataset=dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    return trainer.evaluate()

# âœ… Disable wandb logging to avoid API key issues
import os
os.environ["WANDB_DISABLED"] = "true"

# âœ… Clear GPU memory before evaluation
import torch
torch.cuda.empty_cache()

# âœ… Run evaluation on the smaller subset
fine_tuned_results = evaluate_metrics(hugging_face__model, small_eval_dataset, data_collator, compute_metrics)
print("\nðŸ“ˆ Fine-Tuned Model Results (Subset of 100):")
print(fine_tuned_results)

eval_results = {
    'eval_loss': 45.66122055053711,
    'eval_model_preparation_time': 0.0058,
    'eval_rouge1_f1': 0.15861106107007747,
    'eval_rouge2_f1': 0.0,
    'eval_rougeL_f1': 0.1584279108869273,
    'eval_rougeLsum_f1': 0.15940471186372826,
    'eval_bertscore_f1': 71.25017684698105,
    'eval_runtime': 14.7449,
    'eval_samples_per_second': 6.782,
    'eval_steps_per_second': 6.782
}
results_df = pd.DataFrame([eval_results])
print(results_df)
results_df.to_csv('/content/drive/MyDrive/CAPSTONE/data/eval_metrics_table.csv', index=False)

from reportlab.platypus import SimpleDocTemplate, Table, TableStyle
from reportlab.lib import colors

# Define your metrics
eval_results = {
    'eval_loss': 45.6612,
    'eval_model_preparation_time': 0.0058,
    'eval_rouge1_f1': 0.1586,
    'eval_rouge2_f1': 0.0,
    'eval_rougeL_f1': 0.1584,
    'eval_rougeLsum_f1': 0.1594,
    'eval_bertscore_f1': 71.2502,
    'eval_runtime': 14.7449,
    'eval_samples_per_second': 6.782,
    'eval_steps_per_second': 6.782
}

# Convert to table format
table_data = [["Metric", "Value"]] + [[k, f"{v:.4f}"] for k, v in eval_results.items()]

# Set output path
output_path = "/content/drive/MyDrive/CAPSTONE/data/eval_metrics.pdf"

# Create PDF
pdf = SimpleDocTemplate(output_path)
table = Table(table_data)
table.setStyle(TableStyle([
    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
    ('GRID', (0, 0), (-1, -1), 0.5, colors.black),
    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
]))

pdf.build([table])
print("âœ… PDF saved to:", output_path)

import json

# Load from JSON
with open("generated_summaries_llm.json", "r") as f:
    generated_summaries2 = json.load(f)

print("âœ… Summaries loaded from file")

import pandas as pd

df_results = pd.DataFrame({
    "Original_Feedback": texts,
    "Generated_Summary": generated_summaries2,
    "Labeled_Sentiment": summaries
})

df_results.to_csv("/content/drive/MyDrive/CAPSTONE/data/llm_summary_results.csv", index=False)
print("âœ… Full results saved to llm_summary_results.csv")

def extract_theme(summary):
    prompt = (
        f"Extract a 1â€“2 word theme that best captures the core idea of this summary:\n\n"
        f"Summary: {summary}\n\n"
        f"Respond with only the theme, no punctuation or extra words."
    )
    response = llm_generate(llm_config, [prompt], system_prompt="You are a helpful theme extractor.")
    return response[0].strip().lower()


import pandas as pd

themes = [extract_theme(summary) for summary in generated_summaries2]

theme_df = pd.DataFrame({
    "Sentiment": test_summaries_str[:100],
    "Theme": themes,
    "Summary": generated_summaries2
})

# Get top 6 most frequent themes per sentiment
top_themes = (
    theme_df.groupby("Sentiment")["Theme"]
    .value_counts()
    .groupby(level=0)
    .head(6)
    .reset_index(name="Count")
)
# Pivot into a 6-column format, padding with NaN if fewer than 6 themes
theme_table = top_themes.groupby("Sentiment").apply(
    lambda x: pd.Series(
        list(x["Theme"].values[:6]) + [None] * (6 - len(x["Theme"].values[:6])),
        index=[f"Theme {i+1}" for i in range(6)]
    )
).reset_index()

print(theme_table)

min_len = min(len(test_summaries_str[:100]), len(themes), len(generated_summaries2))

theme_df = pd.DataFrame({
    "Sentiment": test_summaries_str[:min_len],
    "Theme": themes[:min_len],
    "Summary": generated_summaries2[:min_len]
})

# Save the 6-column theme table to CSV
theme_table.to_csv("top_themes_by_sentiment.csv", index=False)
print("âœ… Saved theme table to top_themes_by_sentiment.csv")

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
# Save CSV to your Drive
theme_table.to_csv("/content/drive/MyDrive/CAPSTONE/data/top_themes_by_sentiment.csv", index=False)

# Save chart to your Drive
plt.savefig("/content/drive/MyDrive/CAPSTONE/data/top_themes_bar_chart.png")

def validate_theme_sentiment(theme, sentiment):
    prompt = (
        f"Does the theme '{theme}' align with the sentiment '{sentiment}'?\n"
        f"Respond with 'yes' or 'no'."
    )
    response = llm_generate(llm_config, [prompt], system_prompt="You are a helpful sentiment validator.")
    return response[0].strip().lower() == "yes"

# Melt the theme table to long format for validation
theme_long = theme_table.melt(id_vars="Sentiment", var_name="Theme_Rank", value_name="Theme")

# Drop NaNs
theme_long = theme_long.dropna(subset=["Theme"])

# Validate each theme
theme_long["Valid"] = theme_long.apply(
    lambda row: validate_theme_sentiment(row["Theme"], row["Sentiment"]), axis=1
)

# Keep only valid themes
theme_long_filtered = theme_long[theme_long["Valid"]]

# Pivot back to wide format
theme_table_filtered = (
    theme_long_filtered
    .groupby("Sentiment")["Theme"]
    .apply(lambda x: pd.Series(x.values[:6], index=[f"Theme {i+1}" for i in range(len(x[:6]))]))
    .reset_index()
)

print(theme_table_filtered)

from google.colab import drive
drive.mount('/content/drive')

theme_table_filtered.to_csv("theme_table_filtered.csv", index=False)
print("âœ… Saved theme table to top_themes_by_sentiment.csv")
# Save CSV to your Drive
theme_table_filtered.to_csv("/content/drive/MyDrive/CAPSTONE/data/theme_table_filtered.csv", index=False)

from transformers import pipeline

# Load sentiment analysis pipeline
sentiment_classifier = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")

def simplify_sentiment(label):
    label = label.lower()
    if "positive" in label:
        return "POSITIVE"
    elif "negative" in label:
        return "NEGATIVE"
    else:
        return "NEUTRAL"

# Melt the theme table to long format
theme_long = theme_table.melt(id_vars="Sentiment", var_name="Theme_Rank", value_name="Theme")
theme_long = theme_long.dropna(subset=["Theme"])

# Simplify sentiment labels
theme_long["Simplified_Sentiment"] = theme_long["Sentiment"].apply(simplify_sentiment)

# Classify each theme
theme_long["Predicted_Sentiment"] = theme_long["Theme"].apply(
    lambda x: sentiment_classifier(x)[0]["label"]
)

# Compare predicted vs labeled
theme_long["Valid"] = theme_long["Simplified_Sentiment"] == theme_long["Predicted_Sentiment"]

# Keep only valid themes
theme_long_filtered = theme_long[theme_long["Valid"]]

# Pivot back to wide format
theme_table_filtered2 = (
    theme_long_filtered
    .groupby("Sentiment")["Theme"]
    .apply(lambda x: pd.Series(x.values[:6], index=[f"Theme {i+1}" for i in range(len(x[:6]))]))
    .reset_index()
)

# Save to Drive
theme_table_filtered2.to_csv("/content/drive/MyDrive/CAPSTONE/data/filtered_theme_table2.csv", index=False)
print("âœ… Saved filtered theme table to Drive")

!pip install tabulate

from tabulate import tabulate

print(tabulate(theme_table_filtered2, headers='keys', tablefmt='psql'))
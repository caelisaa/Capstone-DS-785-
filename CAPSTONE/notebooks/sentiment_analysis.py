# -*- coding: utf-8 -*-
"""Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mCPHRRFXWgQPjPSpWSAwSmOO8nGbMgQ1
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set project path
project_path = '/content/drive/MyDrive/CAPSTONE'

import sys
sys.path.append(f'{project_path}/scripts')
from pathlib import Path

# Point to the correct folder in Drive
course_tools_path = Path('/content/drive/MyDrive/CAPSTONE')
sys.path.append(str(course_tools_path))

# Import and run the installer
from install_introdl import ensure_introdl_installed
ensure_introdl_installed(force_update=False, local_path_pkg=course_tools_path / 'introdl')

!pip install bitsandbytes accelerate
!pip install torchinfo

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
from scipy.sparse import csr_matrix

from datasets import load_dataset
from datasets import Dataset

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler

from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel
from transformers import DataCollatorWithPadding
from transformers import TrainingArguments, Trainer
import torch
from torchinfo import summary

from introdl.utils import get_device, wrap_print_text, config_paths_keys
from introdl.nlp import llm_configure, llm_generate, llm_list_models

print = wrap_print_text(print)

sns.set_theme(style='whitegrid')
plt.rcParams['figure.figsize'] = [8, 6]  # Set the default figure size (width, height) in inches

#paths = config_paths_keys(env_path='../Course_Tools/mac.env')
paths = config_paths_keys()
MODELS_PATH = paths['MODELS_PATH']
DATA_PATH = paths['DATA_PATH']

import os
from pathlib import Path

# Define your project root
project_root = Path("/content/drive/MyDrive/CAPSTONE")

# Create a .env file with your Hugging Face token and paths
env_file = project_root / "api_keys.env"
env_file.write_text(f"""\
HF_TOKEN=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
GEMINI_API_KEY=AIzaSyDSj-54ZUx69yrR7_5pYzCRoEyeag0ngdo
MODELS_PATH={project_root}/models
DATA_PATH={project_root}/data
TORCH_HOME={project_root}/models
HF_HOME={project_root}/models
HF_HUB_CACHE={project_root}/models
""")

# Load environment variables from the .env file
from dotenv import load_dotenv
load_dotenv(dotenv_path=env_file)

# Confirm environment variables are set
print("MODELS_PATH =", os.getenv("MODELS_PATH"))
print("DATA_PATH =", os.getenv("DATA_PATH"))

# Authenticate with Hugging Face
from huggingface_hub import login
login(token=os.getenv("HF_TOKEN"))

# Optional: set Hugging Face cache directories explicitly
os.environ["HF_HOME"] = os.getenv("HF_HOME")
os.environ["HF_HUB_CACHE"] = os.getenv("HF_HUB_CACHE")
os.environ["TORCH_HOME"] = os.getenv("TORCH_HOME")

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()

downsample_ratio = 1.0
if downsample_ratio < 1.0:
    df = df.sample(int(downsample_ratio * df.shape[0]))
    print(f'Downsampled dataset has {df.shape[0]} headlines')

# Split data into train and test sets
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(), df["Sentiment"].tolist(), test_size=0.2, random_state=42
)

# Map labels to integer ids
unique_labels = list(set(train_labels_str + test_labels_str))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Convert labels to integer ids as lists
train_ids = [label_to_id[label] for label in train_labels_str]
test_ids = [label_to_id[label] for label in test_labels_str]


tfidf_vectorizer = TfidfVectorizer(
    stop_words='english',  # Remove common English stopwords
    min_df=2,  # Remove rare terms (appear in fewer than 2 documents)
    max_df=0.85,  # Remove overly common terms (appear in more than 85% of documents)
    max_features=1000,  # Limit vocabulary size to top 1000 terms
    ngram_range=(1, 3)  # Include unigrams, bigrams, and trigrams
)

# Fit the vectorizer and transform the input texts
train_features_tfidf = tfidf_vectorizer.fit_transform(train_texts_str)
test_features_tfidf = tfidf_vectorizer.transform(test_texts_str)

model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(train_features_tfidf, train_ids)

# Make predictions on the test set
test_predictions_id = model.predict(test_features_tfidf)

# Convert integer labels to string labels for the test set and predictions
test_predictions_str = [id_to_label[label] for label in test_predictions_id]

# Compute classification metrics
print("Classification Report:")
print(classification_report(test_labels_str, test_predictions_str))

# Compute and
cm = confusion_matrix(test_labels_str, test_predictions_str, labels=list(id_to_label.values()))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(id_to_label.values()))
disp.plot(cmap="Blues", xticks_rotation="vertical")
plt.xticks(rotation=45)
plt.show()


# Create and return a DataFrame with test labels and predictions
results_df = pd.DataFrame({
    "Test Labels": test_labels_str,
    "Predictions": test_predictions_str
})

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment_TEST.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()

downsample_ratio = 1.0
if downsample_ratio < 1.0:
    df = df.sample(int(downsample_ratio * df.shape[0]))
    print(f'Downsampled dataset has {df.shape[0]} headlines')

# Split data into train and test sets
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(), df["Sentiment"].tolist(), test_size=0.2, random_state=42
)

# Map labels to integer ids
unique_labels = list(set(train_labels_str + test_labels_str))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Convert labels to integer ids as lists
train_ids = [label_to_id[label] for label in train_labels_str]
test_ids = [label_to_id[label] for label in test_labels_str]


tfidf_vectorizer = TfidfVectorizer(
    stop_words='english',  # Remove common English stopwords
    min_df=2,  # Remove rare terms (appear in fewer than 2 documents)
    max_df=0.85,  # Remove overly common terms (appear in more than 85% of documents)
    max_features=1000,  # Limit vocabulary size to top 1000 terms
    ngram_range=(1, 3)  # Include unigrams, bigrams, and trigrams
)

# Fit the vectorizer and transform the input texts
train_features_tfidf = tfidf_vectorizer.fit_transform(train_texts_str)
test_features_tfidf = tfidf_vectorizer.transform(test_texts_str)

model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(train_features_tfidf, train_ids)

# Make predictions on the test set
test_predictions_id = model.predict(test_features_tfidf)

# Convert integer labels to string labels for the test set and predictions
test_predictions_str = [id_to_label[label] for label in test_predictions_id]

# Compute classification metrics
print("Classification Report:")
print(classification_report(test_labels_str, test_predictions_str))

# Compute and
cm = confusion_matrix(test_labels_str, test_predictions_str, labels=list(id_to_label.values()))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(id_to_label.values()))
disp.plot(cmap="Blues", xticks_rotation="vertical")
plt.xticks(rotation=45)
plt.show()


# Create and return a DataFrame with test labels and predictions
results_df = pd.DataFrame({
    "Test Labels": test_labels_str,
    "Predictions": test_predictions_str
})

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/First100.csv')
df = df[['Survey_feedback', 'Sentiment']].dropna()

downsample_ratio = 1.0
if downsample_ratio < 1.0:
    df = df.sample(int(downsample_ratio * df.shape[0]))
    print(f'Downsampled dataset has {df.shape[0]} headlines')

# Split data into train and test sets
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Survey_feedback"].tolist(), df["Sentiment"].tolist(), test_size=0.2, random_state=42
)

# Map labels to integer ids
unique_labels = list(set(train_labels_str + test_labels_str))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Convert labels to integer ids as lists
train_ids = [label_to_id[label] for label in train_labels_str]
test_ids = [label_to_id[label] for label in test_labels_str]


tfidf_vectorizer = TfidfVectorizer(
    stop_words='english',  # Remove common English stopwords
    min_df=2,  # Remove rare terms (appear in fewer than 2 documents)
    max_df=0.85,  # Remove overly common terms (appear in more than 85% of documents)
    max_features=1000,  # Limit vocabulary size to top 1000 terms
    ngram_range=(1, 3)  # Include unigrams, bigrams, and trigrams
)

# Fit the vectorizer and transform the input texts
train_features_tfidf = tfidf_vectorizer.fit_transform(train_texts_str)
test_features_tfidf = tfidf_vectorizer.transform(test_texts_str)

model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(train_features_tfidf, train_ids)

# Make predictions on the test set
test_predictions_id = model.predict(test_features_tfidf)

# Convert integer labels to string labels for the test set and predictions
test_predictions_str = [id_to_label[label] for label in test_predictions_id]

# Compute classification metrics
print("Classification Report:")
print(classification_report(test_labels_str, test_predictions_str))

# Compute and
cm = confusion_matrix(test_labels_str, test_predictions_str, labels=list(id_to_label.values()))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(id_to_label.values()))
disp.plot(cmap="Blues", xticks_rotation="vertical")
plt.xticks(rotation=45)
plt.show()


# Create and return a DataFrame with test labels and predictions
results_df = pd.DataFrame({
    "Test Labels": test_labels_str,
    "Predictions": test_predictions_str
})

from llm_helpers import llm_classifier

LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)


def llm_classifier( llm_config,
                    texts,
                    system_prompt,
                    prompt_template,
                    batch_size=1,
                    estimate_cost=False,
                    rate_limit=None):
    """
    Classify text using a Large Language Model (LLM).

    Args:
        llm_config (ModelConfig): Configuration for the LLM.
        texts (list of str): List of text documents to classify.
        system_prompt (str): System prompt to guide the LLM.
        prompt_template (str): Template for user prompts to classify each text.
        batch_size (int): Number of texts to process in a batch for local models.
        estimate_cost (bool): Whether to estimate the cost of the LLM request for API models.
        rate_limit (int): Rate limit per minute for API requests to avoid overloading the LLM service.

    Returns:
        list of str: Predicted labels for the input texts.
    """

    user_prompts = [prompt_template.format(text=text, system_prompt=system_prompt) for text in texts]
    predicted_labels = llm_generate(llm_config,
                                    user_prompts,
                                    system_prompt=system_prompt,
                                    search_strategy='deterministic',
                                    batch_size=batch_size,
                                    estimate_cost=estimate_cost,
                                    rate_limit=rate_limit)

    return predicted_labels

df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()

downsample_ratio = 1.0
if downsample_ratio < 1.0:
    df = df.sample(int(downsample_ratio * df.shape[0]))
    print(f'Downsampled dataset has {df.shape[0]} headlines')

# Split data into train and test sets
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(), df["Sentiment"].tolist(), test_size=0.2, random_state=42
)

# Map labels to integer ids
unique_labels = list(set(train_labels_str + test_labels_str))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Convert labels to integer ids as lists
train_ids = [label_to_id[label] for label in train_labels_str]
test_ids = [label_to_id[label] for label in test_labels_str]

df.head(10)

N = 500
texts = test_texts_str[0:N]
labels = test_labels_str[0:N]

system_prompt = """You are an expert sentiment classifier."""

prompt_template = """{system_prompt}

Using only the following labels:
- Very Positive
- Positive but Critical
- Mixed or Neutral
- Slightly Negative
- Negative
- Very Negative

Classify the following news headline using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else.

Headline: {text}
Label:"""

predictions = llm_classifier( llm_config, texts, system_prompt, prompt_template, batch_size=50)

set(predictions)

filtered_labels = []
filtered_preds = []

for label, pred in zip(labels, predictions):
    pred_clean = pred.strip()
    if pred_clean in label_to_id:
        filtered_labels.append(label_to_id[label])
        filtered_preds.append(label_to_id[pred_clean])
    else:
        print(f"⚠️ Skipping unexpected prediction: {pred_clean}")

labels = filtered_labels
predictions = filtered_preds


# Define the 6 sentiment labels in the correct order
sentiment_labels = [
    "Very Positive",
    "Positive but Critical",
    "Mixed or Neutral",
    "Slightly Negative",
    "Negative",
    "Very Negative"
]

# Define target names
target_names = sentiment_labels


# Generate the classification report
print("Classification Report:")
print(classification_report(labels, predictions, target_names=target_names))

from llm_helpers import llm_classifier

LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

def llm_classifier( llm_config,
                    texts,
                    system_prompt,
                    prompt_template,
                    batch_size=1,
                    estimate_cost=False,
                    rate_limit=None):
    """
    Classify text using a Large Language Model (LLM).

    Args:
        llm_config (ModelConfig): Configuration for the LLM.
        texts (list of str): List of text documents to classify.
        system_prompt (str): System prompt to guide the LLM.
        prompt_template (str): Template for user prompts to classify each text.
        batch_size (int): Number of texts to process in a batch for local models.
        estimate_cost (bool): Whether to estimate the cost of the LLM request for API models.
        rate_limit (int): Rate limit per minute for API requests to avoid overloading the LLM service.

    Returns:
        list of str: Predicted labels for the input texts.
    """

    user_prompts = [prompt_template.format(text=text, system_prompt=system_prompt) for text in texts]
    predicted_labels = llm_generate(llm_config,
                                    user_prompts,
                                    system_prompt=system_prompt,
                                    search_strategy='deterministic',
                                    batch_size=batch_size,
                                    estimate_cost=estimate_cost,
                                    rate_limit=rate_limit)

    return predicted_labels

df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment_TEST.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()

downsample_ratio = 1.0
if downsample_ratio < 1.0:
    df = df.sample(int(downsample_ratio * df.shape[0]))
    print(f'Downsampled dataset has {df.shape[0]} headlines')

# Split data into train and test sets
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(), df["Sentiment"].tolist(), test_size=0.2, random_state=42
)

# Map labels to integer ids
unique_labels = list(set(train_labels_str + test_labels_str))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Convert labels to integer ids as lists
train_ids = [label_to_id[label] for label in train_labels_str]
test_ids = [label_to_id[label] for label in test_labels_str]

df.head(10)

N = 500
texts = test_texts_str[0:N]
labels = test_labels_str[0:N]

system_prompt = """You are an expert sentiment classifier."""

prompt_template = """{system_prompt}

Using only the following labels:
- Positive or Very Positive
- Mixed
- Negative or Very Negative

Classify the following hotel guest comment using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else.

Comment: {text}
Label:"""

predictions = llm_classifier( llm_config, texts, system_prompt, prompt_template, batch_size=50)

set(predictions)

def normalize_prediction(pred):
    pred = pred.strip().lower()
    if "positive" in pred:
        return "Positive or Very Positive"
    elif "mixed" in pred or "neutral" in pred:
        return "Mixed"
    elif "negative" in pred:
        return "Negative or Very Negative"
    else:
        return None

normalized_predictions = [normalize_prediction(pred) for pred in predictions]

# Filter aligned predictions
filtered_labels = []
filtered_preds = []

for label, pred in zip(labels, normalized_predictions):
    if pred in label_to_id:
        filtered_labels.append(label_to_id[label])
        filtered_preds.append(label_to_id[pred])
    else:
        print(f"⚠️ Skipping unexpected prediction: {pred}")

# Final reporting
sentiment_labels = [
    "Positive or Very Positive",
    "Mixed",
    "Negative or Very Negative"
]

target_names = sentiment_labels
label_order = [label_to_id[label] for label in sentiment_labels]

print("✅ Classification Report:")
print(classification_report(filtered_labels, filtered_preds, labels=label_order, target_names=target_names))

from llm_helpers import llm_classifier

# Load and prepare data
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()

# Split data
from sklearn.model_selection import train_test_split
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(),
    df["Sentiment"].tolist(),
    test_size=0.2,
    random_state=42
)

# Define fixed label set
sentiment_labels = [
    "Very Positive",
    "Positive but Critical",
    "Mixed or Neutral",
    "Slightly Negative",
    "Negative",
    "Very Negative"
]

label_to_id = {label: i for i, label in enumerate(sentiment_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Prepare test data
N = 500
texts = test_texts_str[:N]
true_labels = test_labels_str[:N]

# Define prompts
system_prompt = """You are an expert sentiment classifier."""

prompt_template = """{system_prompt}

Using only the following labels:
- Very Positive
- Positive but Critical
- Mixed or Neutral
- Slightly Negative
- Negative
- Very Negative

Classify the following news headline using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else.

Headline: {text}
Label:"""

# Load model
LLM_MODEL = 'unsloth/llama-3-8b-instruct-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

# Run classification
predictions = llm_classifier(
    llm_config,
    texts,
    system_prompt,
    prompt_template,
    batch_size=50
)

# Clean and map predictions
pred_ids = []
true_ids = []

for pred, true_label in zip(predictions, true_labels):
    pred_clean = pred.strip()
    if pred_clean in label_to_id:
        pred_ids.append(label_to_id[pred_clean])
        true_ids.append(label_to_id[true_label])
    else:
        print(f"⚠️ Unexpected prediction: {pred_clean}")
        # Optionally skip or assign fallback
        continue

# Evaluation
print("Classification Report:")
print(classification_report(true_ids, pred_ids, target_names=sentiment_labels))

# Confusion matrix
cm = confusion_matrix(true_ids, pred_ids)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiment_labels)
disp.plot(xticks_rotation=45)
plt.title("Confusion Matrix")
plt.show()

# Load and prepare data
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/First100.csv')
df = df[['Survey_feedback', 'Sentiment']].dropna()

# Split data
from sklearn.model_selection import train_test_split
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Survey_feedback"].tolist(),
    df["Sentiment"].tolist(),
    test_size=0.2,
    random_state=42
)

# Define fixed label set
sentiment_labels = [
    "Very_Positive",
    "Positive_But_Critical",
    "Neutral",
    "Negative",
    "Very_Negative"
]

label_to_id = {label: i for i, label in enumerate(sentiment_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Prepare test data
N = 50
texts = test_texts_str[:N]
true_labels = test_labels_str[:N]

# Define prompts
system_prompt = """You are an expert sentiment classifier."""

prompt_template = """{system_prompt}

Using only the following labels:
- Very_Positive
- Positive_But_Critical
- Neutral
- Negative
- Very_Negative

Classify the following news headline using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else.

Headline: {text}
Label:"""

# Load model
LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

# Run classification
predictions = llm_classifier(
    llm_config,
    texts,
    system_prompt,
    prompt_template,
    batch_size=50
)

# Clean and map predictions
pred_ids = []
true_ids = []

for pred, true_label in zip(predictions, true_labels):
    pred_clean = pred.strip()
    if pred_clean in label_to_id:
        pred_ids.append(label_to_id[pred_clean])
        true_ids.append(label_to_id[true_label])
    else:
        print(f"⚠️ Unexpected prediction: {pred_clean}")
        # Optionally skip or assign fallback
        continue

# Evaluation
print("Classification Report:")
print(classification_report(true_ids, pred_ids, target_names=sentiment_labels))

# Confusion matrix
cm = confusion_matrix(true_ids, pred_ids)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiment_labels)
disp.plot(xticks_rotation=45)
plt.title("Confusion Matrix")
plt.show()

from llm_helpers import llm_classifier

def llm_classifier( llm_config,
                    texts,
                    system_prompt,
                    prompt_template,
                    batch_size=1,
                    estimate_cost=False,
                    rate_limit=None):
    """
    Classify text using a Large Language Model (LLM).

    Args:
        llm_config (ModelConfig): Configuration for the LLM.
        texts (list of str): List of text documents to classify.
        system_prompt (str): System prompt to guide the LLM.
        prompt_template (str): Template for user prompts to classify each text.
        batch_size (int): Number of texts to process in a batch for local models.
        estimate_cost (bool): Whether to estimate the cost of the LLM request for API models.
        rate_limit (int): Rate limit per minute for API requests to avoid overloading the LLM service.

    Returns:
        list of str: Predicted labels for the input texts.
    """

    user_prompts = [prompt_template.format(text=text, system_prompt=system_prompt) for text in texts]
    predicted_labels = llm_generate(llm_config,
                                    user_prompts,
                                    system_prompt=system_prompt,
                                    search_strategy='deterministic',
                                    batch_size=batch_size,
                                    estimate_cost=estimate_cost,
                                    rate_limit=rate_limit)

    return predicted_labels

# Load and prepare data
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/First100.csv')
df = df[['Survey_feedback', 'Sentiment']].dropna()

# Split data
from sklearn.model_selection import train_test_split
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Survey_feedback"].tolist(),
    df["Sentiment"].tolist(),
    test_size=0.2,
    random_state=42
)

# Define fixed label set
sentiment_labels = [
    "Very_Positive",
    "Positive_But_Critical",
    "Neutral",
    "Negative",
    "Very_Negative"
]

label_to_id = {label: i for i, label in enumerate(sentiment_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Prepare test data
N = 50
texts = test_texts_str[:N]
true_labels = test_labels_str[:N]

# Define prompts
system_prompt = """You are an expert sentiment classifier."""

prompt_template = """{system_prompt}

Using only the following labels:
- Very_Positive
- Positive_But_Critical
- Neutral
- Negative
- Very_Negative

Classify the following news headline using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else.

Headline: {text}
Label:"""

# Load model
LLM_MODEL = 'gemini-flash-lite'
llm_config = llm_configure(LLM_MODEL)

# Run classification
predictions = llm_classifier(
    llm_config,
    texts,
    system_prompt,
    prompt_template,
    batch_size=50
)

# Clean and map predictions
pred_ids = []
true_ids = []

for pred, true_label in zip(predictions, true_labels):
    pred_clean = pred.strip()
    if pred_clean in label_to_id:
        pred_ids.append(label_to_id[pred_clean])
        true_ids.append(label_to_id[true_label])
    else:
        print(f"⚠️ Unexpected prediction: {pred_clean}")
        # Optionally skip or assign fallback
        continue

# Evaluation
print("Classification Report:")
print(classification_report(true_ids, pred_ids, target_names=sentiment_labels))

# Confusion matrix
cm = confusion_matrix(true_ids, pred_ids)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiment_labels)
disp.plot(xticks_rotation=45)
plt.title("Confusion Matrix")
plt.show()

# Find three examples from each class in the training set
examples_per_class = 3
few_shot_examples = {}

for text, label in zip(train_texts_str, train_labels_str):
    if len(text.split()) <= 50:
        if label not in few_shot_examples:
            few_shot_examples[label] = []
        if len(few_shot_examples[label]) < examples_per_class:
            few_shot_examples[label].append(text)
        if all(len(examples) == examples_per_class for examples in few_shot_examples.values()):
            break  # Stop once we've filled all classes

# Construct few-shot prompt block
few_shot_prompt_intro = "You are an expert sentiment classifier.\n\n"
few_shot_examples_block = ""

for label, examples in few_shot_examples.items():
    for example in examples:
        few_shot_examples_block += f"Comment: {example}\nPredicted Sentiment: {label}\n\n"

# Final prompt template with placeholders
few_shot_prompt_template = few_shot_prompt_intro + few_shot_examples_block + \
"""
Using only the following labels:
- Very_Positive
- Positive_But_Critical
- Neutral
- Negative
- Very_Negative

Classify the following hotel guest comment using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else.

Comment: {text}
Label:"""

# Load and prepare data
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()

# Split data
from sklearn.model_selection import train_test_split
train_texts_str, test_texts_str, train_labels_str, test_labels_str = train_test_split(
    df["Additional.Feedback.on.Overall.Stay"].tolist(),
    df["Sentiment"].tolist(),
    test_size=0.2,
    random_state=42
)

# Define fixed label set
sentiment_labels = [
    "Very Positive",
    "Positive but Critical",
    "Mixed or Neutral",
    "Slightly Negative",
    "Negative",
    "Very Negative"
]

label_to_id = {label: i for i, label in enumerate(sentiment_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# ✨ Build few-shot prompt with 3 examples per class
examples_per_class = 3
few_shot_examples = {}

for text, label in zip(train_texts_str, train_labels_str):
    if len(text.split()) <= 50:
        if label not in few_shot_examples:
            few_shot_examples[label] = []
        if len(few_shot_examples[label]) < examples_per_class:
            few_shot_examples[label].append(text)
        if all(len(examples) == examples_per_class for examples in few_shot_examples.values()):
            break  # Stop once we've filled all classes

# Construct few-shot prompt block
few_shot_prompt_intro = "You are an expert sentiment classifier.\n\n"
few_shot_examples_block = ""

for label, examples in few_shot_examples.items():
    for example in examples:
        few_shot_examples_block += f"Comment: {example}\nPredicted Sentiment: {label}\n\n"

# Final prompt template with placeholders
few_shot_prompt_template = few_shot_prompt_intro + few_shot_examples_block + \
"""
Using only the following labels:
- Very Positive
- Positive but Critical
- Mixed or Neutral
- Slightly Negative
- Negative
- Very Negative

Classify the following hotel guest comment using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else.

Comment: {text}
Label:"""

# Load model
LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

# Run classification on test set
N = 100
texts = test_texts_str[:N]
true_labels = test_labels_str[:N]

from llm_helpers import llm_classifier

predictions_few_shot = llm_classifier(
    llm_config,
    texts,
    system_prompt="",
    prompt_template=few_shot_prompt_template,
    batch_size=100
)

# Clean and map predictions
pred_ids = []
true_ids = []

for pred, true_label in zip(predictions_few_shot, true_labels):
    pred_clean = pred.strip().replace('"', '').replace("'", "")
    if pred_clean in label_to_id:
        pred_ids.append(label_to_id[pred_clean])
        true_ids.append(label_to_id[true_label])
    else:
        print(f"⚠️ Unexpected prediction: {pred_clean}")


# Evaluation
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

print("📊 Classification Report:")
print(classification_report(true_ids, pred_ids, target_names=sentiment_labels))

cm = confusion_matrix(true_ids, pred_ids)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiment_labels)
disp.plot(xticks_rotation=45)
plt.title("Few-Shot Confusion Matrix")
plt.show()

import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/Hyatt_LTR_Sentiment.csv')
df = df[['Additional.Feedback.on.Overall.Stay', 'Sentiment']].dropna()

# Define sentiment classes
sentiment_labels = [
    "Very Positive",
    "Positive but Critical",
    "Mixed or Neutral",
    "Slightly Negative",
    "Negative",
    "Very Negative"
]

# Map labels
label_to_id = {label: i for i, label in enumerate(sentiment_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Extract data
texts = df["Additional.Feedback.on.Overall.Stay"].tolist()
true_labels = df["Sentiment"].tolist()

# Few-shot setup
examples_per_class = 3
few_shot_examples = {}

for text, label in zip(texts, true_labels):
    label_clean = label.strip().title()
    if len(text.split()) <= 50:
        if label_clean not in few_shot_examples:
            few_shot_examples[label_clean] = []
        if len(few_shot_examples[label_clean]) < examples_per_class:
            few_shot_examples[label_clean].append(text)
        if all(len(few_shot_examples.get(lbl, [])) == examples_per_class for lbl in sentiment_labels):
            break

# Build few-shot prompt
few_shot_prompt_template = "You are an expert sentiment classifier for hotel guest feedback.\n\n"

for label in sentiment_labels:
    for example in few_shot_examples.get(label, []):
        few_shot_prompt_template += f"Feedback: {example}\nPredicted Sentiment: {label}\n\n"

# Refined classification instruction
few_shot_prompt_template += """Carefully analyze the tone and content of the feedback.
Use the following labels based on clear guidelines:

- Very Negative: Strong emotional complaints, multiple major issues, guest explicitly upset (e.g., “worst experience”, “furious”, “never returning”).
- Negative: Clear dissatisfaction or service failure, but without emotional intensity.
- Slightly Negative: One minor issue mentioned (e.g., “pillows were flat”), otherwise polite or neutral tone.
- Mixed or Neutral: Balanced or vague tone, may mention both good and bad without clear emotion (e.g., “it was fine”, “okay overall”).
- Positive but Critical: Generally positive with small critique or suggestions (e.g., “room was lovely but breakfast was mediocre”).
- Very Positive: Enthusiastic praise, guest clearly satisfied with no complaints.

Return only one of the following labels:
- Very Positive
- Positive but Critical
- Mixed or Neutral
- Slightly Negative
- Negative
- Very Negative

Do not use any other labels or add punctuation/explanation.

Feedback: {text}
Predicted Sentiment:"""

# Load model
LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

# Sample test data
from llm_helpers import llm_classifier
N = 100
texts_sample = texts[:N]
labels_sample = true_labels[:N]

# Run predictions
predictions_few_shot = llm_classifier(
    llm_config,
    texts_sample,
    system_prompt="",
    prompt_template=few_shot_prompt_template,
    batch_size=10
)

# Fix formatting issues
def remap_prediction(pred):
    pred = pred.strip().replace("_", " ").replace('"', '').replace("'", "").replace("\n", "").title()
    if pred.lower() == "positive":
        return "Positive but Critical"
    if pred.lower() == "mixed":
        return "Mixed or Neutral"
    for label in sentiment_labels:
        if pred.lower() == label.lower():
            return label
    return None

# Align predictions
filtered_labels = []
filtered_predictions = []

for true, pred in zip(labels_sample, predictions_few_shot):
    true_clean = remap_prediction(true)
    pred_clean = remap_prediction(pred)
    if pred_clean and true_clean in label_to_id:
        filtered_labels.append(label_to_id[true_clean])
        filtered_predictions.append(label_to_id[pred_clean])
    else:
        print(f"⚠️ Skipping invalid prediction: '{pred}' or true label: '{true}'")

# Report
label_order = [label_to_id[label] for label in sentiment_labels]

print("\n📊 Classification Report:")
print(classification_report(
    filtered_labels,
    filtered_predictions,
    labels=label_order,
    target_names=sentiment_labels
))

cm = confusion_matrix(filtered_labels, filtered_predictions, labels=label_order)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiment_labels)
disp.plot(xticks_rotation=45)
plt.title("Few-Shot Confusion Matrix")
plt.show()

import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/First100.csv')
df = df[['Survey_feedback', 'Sentiment']].dropna()

# Define sentiment classes with space-based formatting to match model output
sentiment_labels = [
    "Very Negative",
    "Negative",
    "Neutral",
    "Positive But Critical",
    "Very Positive"
]

# Map labels to IDs
label_to_id = {label: i for i, label in enumerate(sentiment_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Extract texts and labels
texts = df["Survey_feedback"].tolist()
true_labels = df["Sentiment"].tolist()

# Build few-shot examples
examples_per_class = 3
few_shot_examples = {}

for text, label in zip(texts, true_labels):
    label_clean = label.strip().replace("_", " ").replace('"', '').replace("'", "").title()
    if len(text.split()) <= 50:
        if label_clean not in few_shot_examples:
            few_shot_examples[label_clean] = []
        if len(few_shot_examples[label_clean]) < examples_per_class:
            few_shot_examples[label_clean].append(text)
        if all(len(few_shot_examples.get(lbl, [])) == examples_per_class for lbl in sentiment_labels):
            break

# Construct few-shot prompt
few_shot_prompt_template = "You are an expert sentiment classifier for hotel guest feedback.\n\n"

for label in sentiment_labels:
    for example in few_shot_examples.get(label, []):
        few_shot_prompt_template += f"Feedback: {example}\nPredicted Sentiment: {label}\n\n"

# Add strict instruction
few_shot_prompt_template += """Carefully analyze the tone and content of the feedback.
If the guest mentions any dissatisfaction, complaint, inconvenience, or frustration — this likely indicates Neutral, Negative or Very Negative sentiment.
If the guest mentions all good things, it is Very Positive.
Do not assume feedback is neutral or positive just because it is polite or brief.
Be strict and cautious: if there is any indication of a problem, lean toward a negative label.

Use only one of the following labels:
- Very Positive
- Positive But Critical
- Neutral
- Negative
- Very Negative

Classify the following hotel guest feedback using exactly one of the labels above.
Do not use any other labels.
You must return only the label with nothing else — no punctuation, no explanation.

Feedback: {text}
Predicted Sentiment:"""

# Load model
LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

# Run prediction
system_prompt = ""
predictions_few_shot = llm_classifier(
    llm_config,
    texts,
    system_prompt,
    few_shot_prompt_template,
    batch_size=5
)

# Normalize and remap predictions
def remap_prediction(pred):
    pred = pred.strip().replace("_", " ").replace('"', '').replace("'", "").title()
    if pred == "Positive":
        return "Positive But Critical"
    if pred in sentiment_labels:
        return pred
    return None

# Filter and align predictions
filtered_labels = []
filtered_predictions = []

for true_label, pred in zip(true_labels, predictions_few_shot):
    true_label_clean = true_label.strip().replace("_", " ").replace('"', '').replace("'", "").title()
    pred_clean = remap_prediction(pred)
    if pred_clean and true_label_clean in label_to_id:
        filtered_labels.append(label_to_id[true_label_clean])
        filtered_predictions.append(label_to_id[pred_clean])
    else:
        print(f"⚠️ Skipping invalid prediction: '{pred}' or true label: '{true_label}'")

# Evaluation
label_order = [label_to_id[label] for label in sentiment_labels]

print("\n📊 Classification Report:")
print(classification_report(
    filtered_labels,
    filtered_predictions,
    labels=label_order,
    target_names=sentiment_labels
))

cm = confusion_matrix(filtered_labels, filtered_predictions, labels=label_order)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiment_labels)
disp.plot(xticks_rotation=45)
plt.title("Few-Shot Confusion Matrix")
plt.show()

import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('/content/drive/MyDrive/CAPSTONE/data/First100.csv')
df = df[['Survey_feedback', 'Sentiment']].dropna()

# Define sentiment classes with space-based formatting to match model output
sentiment_labels = [
    "Very Negative",
    "Negative",
    "Neutral",
    "Positive But Critical",
    "Very Positive"
]

# Map labels to IDs
label_to_id = {label: i for i, label in enumerate(sentiment_labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

# Extract texts and labels
texts = df["Survey_feedback"].tolist()
true_labels = df["Sentiment"].tolist()

# Build few-shot examples
examples_per_class = 3
few_shot_examples = {}

for text, label in zip(texts, true_labels):
    label_clean = label.strip().replace("_", " ").replace('"', '').replace("'", "").title()
    if len(text.split()) <= 50:
        if label_clean not in few_shot_examples:
            few_shot_examples[label_clean] = []
        if len(few_shot_examples[label_clean]) < examples_per_class:
            few_shot_examples[label_clean].append(text)
        if all(len(few_shot_examples.get(lbl, [])) == examples_per_class for lbl in sentiment_labels):
            break

# Construct few-shot prompt
few_shot_prompt_template = "You are an expert sentiment classifier for hotel guest feedback.\n\n"

for label in sentiment_labels:
    for example in few_shot_examples.get(label, []):
        few_shot_prompt_template += f"Feedback: {example}\nPredicted Sentiment: {label}\n\n"

# Add strict instruction
few_shot_prompt_template += """You are an expert sentiment classifier for hotel guest feedback.

Your job is to carefully analyze the tone, phrasing, and emotional weight of each comment. Choose from the following sentiment labels based on clear criteria:

- Very Negative
- Negative
- Neutral
- Positive But Critical
- Very Positive

Definitions and Decision Criteria:

- Very Negative: Feedback with emotionally charged language, multiple severe complaints, or strong dissatisfaction. Look for phrases like “worst experience”, “disgusting”, “furious”, “never coming back”.
- Negative: Clear dissatisfaction, inconvenience, or disappointment, but not extreme or emotionally intense.
- Neutral: Comments that mention both good and bad, are informational in tone, or lack clear sentiment. Often polite, brief, or balanced without emotion.
- Positive But Critical: Feedback that is overall positive but includes minor complaints, improvement suggestions, or constructive critique.
- Very Positive: Fully satisfied guests. Comments are enthusiastic and contain praise without criticism.

Important Reminders:
- Do not assume feedback is neutral or positive just because it’s short or polite.
- If there is a complaint, even if wrapped in nice language, do not label it as “Very Positive”.
- If no sentiment is obvious, lean toward “Neutral”.
- Only use labels above. Return only the label — no punctuation, no explanation.

Only return the label. If tempted to add extra context, commentary, or explanations — do not. Output must be only one of the five labels above.

Feedback: {text}
Predicted Sentiment:"""


# Load model
LLM_MODEL = 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit'
llm_config = llm_configure(LLM_MODEL)

# Run prediction
system_prompt = ""
predictions_few_shot = llm_classifier(
    llm_config,
    texts,
    system_prompt,
    few_shot_prompt_template,
    batch_size=5
)

# Normalize and remap predictions
def remap_prediction(pred):
    pred = pred.strip().replace("_", " ").replace('"', '').replace("'", "").title()
    if pred == "Positive":
        return "Positive But Critical"
    if pred in sentiment_labels:
        return pred
    return None

# Filter and align predictions
filtered_labels = []
filtered_predictions = []

for true_label, pred in zip(true_labels, predictions_few_shot):
    true_label_clean = true_label.strip().replace("_", " ").replace('"', '').replace("'", "").title()
    pred_clean = remap_prediction(pred)
    if pred_clean and true_label_clean in label_to_id:
        filtered_labels.append(label_to_id[true_label_clean])
        filtered_predictions.append(label_to_id[pred_clean])
    else:
        print(f"⚠️ Skipping invalid prediction: '{pred}' or true label: '{true_label}'")

# Evaluation
label_order = [label_to_id[label] for label in sentiment_labels]

print("\n📊 Classification Report:")
print(classification_report(
    filtered_labels,
    filtered_predictions,
    labels=label_order,
    target_names=sentiment_labels
))

cm = confusion_matrix(filtered_labels, filtered_predictions, labels=label_order)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sentiment_labels)
disp.plot(xticks_rotation=45)
plt.title("Few-Shot Confusion Matrix")
plt.show()